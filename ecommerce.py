# -*- coding: utf-8 -*-
"""ecommerce.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11-PycHzZpNbaA-g4-z2spEvZdTBZYgb6
"""

import pandas as pd
import numpy as np
from scipy.stats import chi2_contingency
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score,mean_squared_error,accuracy_score
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
import random

df = pd.read_csv('train.csv')

# df

#df.head()

# df.groupby('Offer %')['fulfilled1'].value_counts()

# # Assuming df is your DataFrame and it has columns 'platform' and 'fulfilled1'
# contingency_table = pd.crosstab(df['Offer %'], df['fulfilled1'])

# # Perform the Chi-Square test
# chi2, p, dof, expected = chi2_contingency(contingency_table)

# print(f"Chi-Square Statistic : {chi2}")
# print(f"P-value : {p}")

# plt.bar(df['Offer %'], df['fulfilled1'])

# df['star_5f'].value_counts()

"""Drop Column offer %"""

df.drop(['Offer %'],inplace=True,axis=1)

"""Drop Column Id"""

df.drop(['id'],inplace=True,axis=1)

"""Drop Column title"""

df.drop(['title'],inplace=True,axis=1)

"""Clear Null Values"""

# df.isnull().sum()

# df.norating1.value_counts()

df['maincateg'] = df['maincateg'].fillna('Women')

# df.head(20)

# df['noreviews1'].value_counts()

# (df['star_3f'] == 34.0).sum()

df['norating1'].fillna(df['norating1'].median(), inplace=True)

df['noreviews1'].fillna(df['noreviews1'].median(), inplace=True)

df['star_3f'].fillna(df['star_3f'].median(), inplace=True)

df['star_5f'].fillna(df['star_5f'].median(), inplace=True)

df['star_4f'].fillna(df['star_4f'].median(), inplace=True)

df['star_3f'].fillna(df['star_3f'].median(), inplace=True)

# df['star_3f'].median()

"""Drop duplicate rows"""

df.drop_duplicates(inplace=True)

"""Handling outliers"""

# df.head()

# plt.boxplot(df['star_1f'])

q1_rating = df['Rating'].quantile(0.25)
q3_rating = df['Rating'].quantile(0.75)
IQR = q3_rating - q1_rating

lower_limit_rating = q1_rating - 1.5 * IQR
upper_limit_rating = q3_rating + 1.5 * IQR

df['Rating'] = df['Rating'].clip(lower=lower_limit_rating,upper=upper_limit_rating)

q1_price1 = df['price1'].quantile(0.25)
q3_price1 = df['price1'].quantile(0.75)
IQR = q3_price1 - q1_price1

lower_limit_price1 = q1_price1 - 1.5 * IQR
upper_limit_price1 = q3_price1 + 1.5 * IQR

df['price1'] = df['price1'].clip(lower=lower_limit_price1,upper=upper_limit_price1)

q1_actprice1 = df['actprice1'].quantile(0.25)
q3_actprice1 = df['actprice1'].quantile(0.75)
IQR = q3_actprice1 - q1_actprice1

lower_limit_actprice1 = q1_actprice1 - 1.5 * IQR
upper_limit_actprice1 = q3_actprice1 + 1.5 * IQR

df['actprice1'] = df['actprice1'].clip(lower=lower_limit_actprice1,upper=upper_limit_actprice1)

q1_norating1 = df['norating1'].quantile(0.25)
q3_norating1 = df['norating1'].quantile(0.75)
IQR = q3_norating1 - q1_norating1

lower_limit_norating1 = q1_norating1 - 1.5 * IQR
upper_limit_norating1 = q3_norating1 + 1.5 * IQR

df['norating1'] = df['norating1'].clip(lower=lower_limit_norating1,upper=upper_limit_norating1)

q1_noreviews1 = df['noreviews1'].quantile(0.25)
q3_noreviews1 = df['noreviews1'].quantile(0.75)
IQR = q3_noreviews1 - q1_noreviews1

lower_limit_noreviews1 = q1_noreviews1 - 1.5 * IQR
upper_limit_noreviews1 = q3_noreviews1 + 1.5 * IQR

df['noreviews1'] = df['noreviews1'].clip(lower=lower_limit_noreviews1,upper=upper_limit_noreviews1)

q1_star_5f = df['star_5f'].quantile(0.25)
q3_star_5f = df['star_5f'].quantile(0.75)
IQR = q3_star_5f - q1_star_5f

lower_limit_star_5f = q1_star_5f - 1.5 * IQR
upper_limit_star_5f = q3_star_5f + 1.5 * IQR

df['star_5f'] = df['star_5f'].clip(lower=lower_limit_star_5f,upper=upper_limit_star_5f)

q1_star_4f = df['star_4f'].quantile(0.25)
q3_star_4f = df['star_4f'].quantile(0.75)
IQR = q3_star_4f - q1_star_4f

lower_limit_star_4f = q1_star_4f - 1.5 * IQR
upper_limit_star_4f = q3_star_4f + 1.5 * IQR

df['star_4f'] = df['star_4f'].clip(lower=lower_limit_star_4f,upper=upper_limit_star_4f)

q1_star_3f = df['star_3f'].quantile(0.25)
q3_star_3f = df['star_3f'].quantile(0.75)
IQR = q3_star_3f - q1_star_3f

lower_limit_star_3f = q1_star_3f - 1.5 * IQR
upper_limit_star_3f = q3_star_3f + 1.5 * IQR

df['star_3f'] = df['star_3f'].clip(lower=lower_limit_star_3f,upper=upper_limit_star_3f)

q1_star_2f = df['star_2f'].quantile(0.25)
q3_star_2f = df['star_2f'].quantile(0.75)
IQR = q3_star_2f - q1_star_2f

lower_limit_star_2f = q1_star_2f - 1.5 * IQR
upper_limit_star_2f = q3_star_2f + 1.5 * IQR

df['star_2f'] = df['star_2f'].clip(lower=lower_limit_star_2f,upper=upper_limit_star_2f)

q1_star_1f = df['star_1f'].quantile(0.25)
q3_star_1f = df['star_1f'].quantile(0.75)
IQR = q3_star_1f - q1_star_1f

lower_limit_star_1f = q1_star_1f - 1.5 * IQR
upper_limit_star_1f = q3_star_1f + 1.5 * IQR

df['star_1f'] = df['star_1f'].clip(lower=lower_limit_star_1f,upper=upper_limit_star_1f)

# df.boxplot()

"""Encoding"""

df.head()

df = pd.concat([df,pd.get_dummies(df['maincateg'])],axis=1)

df.drop('maincateg',inplace=True,axis=1)

label_encoder = LabelEncoder()
df['platform'] = label_encoder.fit_transform(df['platform'])

df

df['price1'].value_counts()

# Assuming df is your DataFrame
# df['sorted_price1'] = df['price1'].sort_values().values
# df['sorted_price1'].value_counts()

# df.groupby('sorted_price1')['fulfilled1'].value_counts()

# df['price_range'] = pd.qcut(df['sorted_price1'], 3, labels=['low', 'medium', 'high'])

# df['price_range'].value_counts()

# df.groupby('price_range')['fulfilled1'].value_counts()

df

df['Rating'].min()

"""Scaling

"""

# from scipy import stats

# # Assuming data is a 1-D numpy array or list containing your data
# shapiro_test = stats.shapiro(df['norating1'])
# ks_test = stats.kstest(df['norating1'], 'norm')
# anderson_test = stats.anderson(df['norating1'], 'norm')

# print(f"Shapiro-Wilk Test: {shapiro_test}")
# print(f"Kolmogorov-Smirnov Test: {ks_test}")
# print(f"Anderson-Darling Test: {anderson_test}")

minmax_scaler = MinMaxScaler()
df['price1'] = minmax_scaler.fit_transform(df['price1'].values.reshape(-1, 1))
df['actprice1'] = minmax_scaler.fit_transform(df['actprice1'].values.reshape(-1, 1))
df['norating1'] = minmax_scaler.fit_transform(df['norating1'].values.reshape(-1, 1))
df['noreviews1'] = minmax_scaler.fit_transform(df['noreviews1'].values.reshape(-1, 1))
df['star_5f'] = minmax_scaler.fit_transform(df['star_5f'].values.reshape(-1, 1))
df['star_4f'] = minmax_scaler.fit_transform(df['star_4f'].values.reshape(-1, 1))
df['star_3f'] = minmax_scaler.fit_transform(df['star_3f'].values.reshape(-1, 1))
df['star_2f'] = minmax_scaler.fit_transform(df['star_2f'].values.reshape(-1, 1))
df['star_1f'] = minmax_scaler.fit_transform(df['star_1f'].values.reshape(-1, 1))
df['Rating'] = minmax_scaler.fit_transform(df['Rating'].values.reshape(-1, 1))

df

"""Machine learning"""

x = df.drop(['fulfilled1','Women'],axis=1)
y = df['fulfilled1']

x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=44,test_size=0.20)

"""RandomForestClassifier"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [490,500,510],
    'max_depth': [12,21,26],
    'min_samples_split': [4,5,6],
    'min_samples_leaf': [0,1,3]
}
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(x_train, y_train)

best_params_rfc = grid_search.best_params_
#{'max_depth': 21, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 500}

print(best_params_rfc)
#Accuracy is 0.7975224263135412
#{'max_depth': 21, 'min_samples_leaf': 1, 'min_samples_split': 4, 'n_estimators': 500}

rfc = RandomForestClassifier()
rfc.fit(x_train,y_train)
y_pred = rfc.predict(x_test)
print(f"Accuracy is {accuracy_score(y_test,y_pred)}")
#Accuracy is 0.7931807985643786

"""XGB"""

import xgboost as xgb
xgb_model = xgb.XGBClassifier(objective='binary:logistic')

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 0.9, 1.0],
    'colsample_bytree': [0.8, 0.9, 1.0],
}

grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy')
grid_search.fit(x_train, y_train)

best_params_xgb = grid_search.best_params_

print(best_params_xgb)
#Accuracy is 0.7891431135038134

xgb_model = xgb.XGBClassifier(objective='binary:logistic',**best_params_xgb)
xgb_model.fit(x_train,y_train)
y_pred = xgb_model.predict(x_test)
print(f"Accuracy is {accuracy_score(y_test,y_pred)}")

# # !pip install tensorflow-addons
# # !pip install scikit-learn
# # !pip install --upgrade tensorflow scikit-learn
# # !pip install --upgrade tensorflow
# # # !pip uninstall tensorflow
# # # !pip install tensorflow

# import numpy as np
# from sklearn.model_selection import GridSearchCV
# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
# # from scikeras.wrappers import KerasClassifier

# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense
# from tensorflow.keras.optimizers import Adam

# # Function to create a Keras model
# def create_model(learning_rate=0.01, units1=64, units2=32):
#     model = Sequential()
#     model.add(Dense(units1, input_dim=x_train.shape[1], activation='relu'))
#     model.add(Dense(units2, activation='relu'))
#     model.add(Dense(1, activation='sigmoid'))
#     model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])
#     return model

# # Create KerasClassifier
# keras_classifier = KerasClassifier(build_fn=create_model, epochs=25, batch_size=42, verbose=0)

# # Define hyperparameter grid
# param_grid = {
#     'learning_rate': [0.01, 0.1, 0.2],
#     'units1': [32, 64, 128],
#     'units2': [16, 32, 64]
# }

# # Use GridSearchCV to find the best hyperparameters
# grid_search = GridSearchCV(estimator=keras_classifier, param_grid=param_grid, cv=3, scoring='accuracy')
# grid_result = grid_search.fit(x_train, y_train)

# # Print the best hyperparameters
# print("Best Hyperparameters: ", grid_result.best_params_)

# # Evaluate the model with the best hyperparameters on the test set
# best_model = grid_result.best_estimator_
# test_loss, test_accuracy = best_model.model.evaluate(x_test, y_test)
# print(f'\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')

"""Neural Network"""

# # Install Keras Tuner if not installed
# # !pip install keras-tuner

# import numpy as np
# import tensorflow as tf
# from tensorflow import keras
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense
# from tensorflow.keras.optimizers import Adam
# from kerastuner.tuners import RandomSearch

# # Function to create a Keras model
# def build_model(hp):
#     model = Sequential()
#     model.add(Dense(units=hp.Int('units1', min_value=32, max_value=256, step=32),
#                     input_dim=x_train.shape[1], activation='relu'))
#     model.add(Dense(units=hp.Int('units2', min_value=16, max_value=128, step=16), activation='relu'))
#     model.add(Dense(1, activation='sigmoid'))

#     learning_rate = hp.Choice('learning_rate', values=[0.1, 0.2,0.4,0.5])

#     model.compile(loss='binary_crossentropy',
#                   optimizer=Adam(learning_rate=learning_rate),
#                   metrics=['accuracy'])

#     return model

# # Create Keras Tuner RandomSearch
# tuner = RandomSearch(
#     build_model,
#     objective='val_accuracy',
#     max_trials=12,  # Number of hyperparameter combinations to try
#     executions_per_trial=4,  # Number of models to train per trial
#     directory='my_tuner_directory',  # Directory to store the results
#     project_name='my_tuning_project'  # Name of the tuning project
# )

# # Perform hyperparameter search
# tuner.search(x_train, y_train, epochs=15, validation_data=(x_test, y_test))

# # Get the best hyperparameters
# best_hps = tuner.get_best_hyperparameters(num_trials=10)[0]

# # Build the final model with the best hyperparameters
# final_model = tuner.hypermodel.build(best_hps)

# # Train the final model
# final_model.fit(x_train, y_train, epochs=15, validation_data=(x_test, y_test))

# # Evaluate the final model
# test_loss, test_accuracy = final_model.evaluate(x_test, y_test)
# print(f'\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')

"""AdaBoostClassifier"""

# Import necessary libraries
from sklearn.ensemble import AdaBoostClassifier


# base_model = RandomForestClassifier(max_depth=15)

# Initialize AdaBoost classifier with the base model
adaboost_model = AdaBoostClassifier(rfc,learning_rate=0.90,n_estimators=70, random_state=42)

# Train the AdaBoost model
adaboost_model.fit(x_train, y_train)

# Make predictions on the test set
predictions = adaboost_model.predict(x_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, predictions)
print(f"Accuracy: {accuracy }")